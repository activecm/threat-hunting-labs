[{"uri":"/basic_usage/","title":"Basic Tool Usage","tags":[],"description":"","content":" Zeek Process a Pcap The following command will output Zeek logs in the current directory. Because of this we recommend creating a new directory first, in this case the logs directory.\nmkdir logs cd logs  Next, modify the following command to give the correct path to your pcap file. You only need to change the pcap path. Do not change the word local.\nzeek -r /path/to/sample.pcap local  The Zeek arguments are:\n -r /path/to/sample.pcap is the path to the pcap you want to process. local is the name of the Zeek script to use to process the pcap.  Depending on what you are doing next you may want to either stay in the current directory to work directly with the newly generated Zeek logs, or you may want change back to your original directory.\ncd ..  Here is a sample of the logs generated that you can find in your logs directory:\n conn.log - Connection information for TCP, UDP, and ICMP dns.log - DNS requests \u0026amp; responses http.log - HTTP requests ssl.log - SSL/TLS certificate information  You can find more information about these logs in Zeek\u0026rsquo;s documentation or in Corelight\u0026rsquo;s Cheatsheets.\nRITA Import Zeek Logs The following command assumes you have your Zeek logs in the logs directory and you want to name your dataset sample.\nrita import logs sample  Viewing Results In general, the syntax for viewing results is:\nrita COMMAND dataset  Where COMMAND is one of:\n show-beacons - Print hosts which show signs of beaconing activity show-exploded-dns - Print dns analysis. Exposes covert dns channels show-long-connections - Print long connections and relevant information show-strobes - Print strobe information show-useragents - Print user agent information  These are the commands you will find most useful in these labs. You can run rita --help to see the full list of commands you can use.\nYou can also run rita COMMAND --help to view detailed usage for each command. Most of the commands support these options:\n -H, --human-readable - Prints the results in an ASCII table that is easier to read on the command line. By default RITA outputs CSV format that is suitable for importing into a spreadsheet or further processing with the likes of grep, cut, and sort. --limit 10 - Sets the number of results that are output. This is equivalent to piping to tail but will return results quicker. --no-limit - By default the limit is set to 1000 but if you wish to see all results you can use this flag.  HTML Report Run the following command on your dataset. In this case the dataset name is \u0026ldquo;sample\u0026rdquo;.\nrita html-report sample  This will generate HTML files with the results from the dataset. It should open the report in a web browser for you. But if you need to you can open this file manually: sample/index.html\nClick on the dataset name \u0026ldquo;sample\u0026rdquo; and you should see this screen.\nNavigate between the different reports using the labels on the top of the screen. For instance, clicking Beacons will display a table with the beacon results in it.\n"},{"uri":"/long_connections/","title":"Long Connections","tags":[],"description":"","content":" Goal Identify the long-lived network connections.\nTools Used  Wireshark Zeek RITA  Background To determine how long a connection stays open, we first need to define what constitutes the start or end of a connection.\nFor TCP, a typical connection starts with a 3-way handshake (SYN, SYN/ACK, ACK) and ends with a 4-way handshake (FIN, ACK, FIN, ACK)\nTCP 3-Way Handshake Connection Start\nsequenceDiagram Client-\u0026gt;\u0026gt;Server: SYN Server-\u0026gt;\u0026gt;Client: SYN/ACK Client-\u0026gt;\u0026gt;Server: ACK  TCP 4 -Way Handshake Connection End\nsequenceDiagram Client-\u0026gt;\u0026gt;Server: FIN Server-\u0026gt;\u0026gt;Client: ACK Server-\u0026gt;\u0026gt;Client: FIN Client-\u0026gt;\u0026gt;Server: ACK  This means that it is easy to tell when a TCP connection starts and ends. However, stateless protocols (namely UDP) do not have the same property. There is no official opening to a UDP connection, and the term \u0026ldquo;connection\u0026rdquo; can be tricky to apply to UDP at all. What firewalls and most packet analysis tools do is define a time window (typically 30-60 seconds) during which UDP packets using the same IPs and port numbers are considered part of the same \u0026ldquo;session\u0026rdquo;. Each time a new packet is seen the session window TCPtimer is reset. This means that a session is considered started when the first UDP packet is seen and ended when no more UDP packets have been seen for the duration of the time window. The terms \u0026ldquo;session\u0026rdquo; and \u0026ldquo;connection\u0026rdquo; are often used interchangeably.\nThe following information must remain the same across packets to be considered part of the same session. Though the IP and ports will swap between source and destination depending on which direction the packet is going. * source and destination IP addresses * source and destination ports * protocol\nCollectively, these pieces of information are commonly referred to as a 5-tuple and used to distinguish individual sessions.\nHunt We\u0026rsquo;re going to find the connections that have been active for the longest amount of time. These could indicate an attacker with a long-lived C2 session.\nWireshark Open your pcap in Wireshark. This loads every individual packet in the main window.\nUnder the Statistics menu select Conversations. The Conversations window summarizes all the packets into \u0026ldquo;conversations\u0026rdquo;.\nSelect the TCP tab and sort by clicking on the Duration column. Click it again to sort in decreasing order.\nFrom here you can see which TCP connections were held open for the longest. In the image above we have a connection from 10.55.100.100 to 65.52.108.255 on port 443 (HTTPS) that was open for 86,222 seconds, or 23.95 hours.\nNext, select the UDP tab and apply the same sort by clicking on the Duration column twice.\nHere we have a UDP packets from 192.168.88.2 to 216.299.4.69 on port 123 (NTP) for a duration of 86,217 seconds, or 23.95 hours.\nHowever, recall the discussion above about how UDP does not have real \u0026ldquo;start\u0026rdquo; and \u0026ldquo;end\u0026rdquo; times. Some tools will use a time window to determine when a UDP connection ends. We can see that Wireshark most likely does not do this by looking at the second entry. If you take the total duration (86,209) and divide it by the total number of packets sent (6) we see that there was on average a delay of 14,368 seconds (or about 4 hours) between packets. A time window that broad is useless and it is safe to assume that Wireshark is showing us \u0026ldquo;conversations\u0026rdquo; with data from the entire timespan of the packet capture.\nZeek Be sure to analyze your pcap using Zeek before starting.\nYour Zeek logs should include a file called conn.log. You can inspect what\u0026rsquo;s in this file using the head command.\nhead conn.log  #separator \\x09 #set_separator\t, #empty_field\t(empty) #unset_field\t- #path\tconn #open\t2019-10-16-15-13-09 #fields\tts\tuid\tid.orig_h\tid.orig_p\tid.resp_h\tid.resp_p\tproto\tserviceduration\torig_bytes\tresp_bytes\tconn_state\tlocal_orig\tlocal_resp\tmissed_bytes\thistory\torig_pkts\torig_ip_bytes\tresp_pkts\tresp_ip_bytes\ttunnel_parents #types\ttime\tstring\taddr\tport\taddr\tport\tenum\tstring\tinterval\tcount\tcount\tstring\tbool\tbool\tcount\tstring\tcount\tcount\tcount\tcount\tset[string] 1517336042.090842\tCW32gzposD5TUDUB\t10.55.182.100\t14291\t10.233.233.5\t80\ttcp\t-3.000158\t0\t0\tS0\t-\t-\t0\tS\t2\t104\t0\t0\t- 1517336042.279652\tComPBK1vso3uDC8KS2\t192.168.88.2\t55638\t165.227.88.15\t53\tudp\tdns\t0.069982\t61\t81\tSF\t-\t-\t0\tDd\t1\t89\t1\t109\t-  This isn\u0026rsquo;t very readable on it\u0026rsquo;s own. There are too many columns to display on a single line. Let\u0026rsquo;s use zeek-cut to reduce the columns to what we\u0026rsquo;d like to look at.\nhead conn.log | zeek-cut -c id.orig_h id.orig_p id.resp_h id.resp_p proto service duration  #separator \\x09 #set_separator\t, #empty_field\t(empty) #unset_field\t- #path\tconn #open\t2019-10-16-15-13-09 #fields\tid.orig_h\tid.orig_p\tid.resp_h\tid.resp_p\tproto\tservice\tduration #types\taddr\tport\taddr\tport\tenum\tstring\tinterval 10.55.182.100\t14291\t10.233.233.5\t80\ttcp\t-\t3.000158 192.168.88.2\t55638\t165.227.88.15\t53\tudp\tdns\t0.069982  This is better, but so far we are only processing the first few lines of the file. Using cat instead of head will show the entire file scroll by.\ncat conn.log | zeek-cut -c id.orig_h id.orig_p id.resp_h id.resp_p proto service duration  ... 10.55.100.111\t57481\t172.217.6.2\t443\ttcp\tssl\t105.310590 10.55.100.111\t57475\t198.160.127.57\t443\ttcp\tssl\t105.614383 10.55.100.107\t51651\t52.203.62.126\t443\ttcp\tssl\t14.495950 10.55.100.111\t57447\t54.215.180.145\t443\ttcp\tssl\t109.111123 10.55.100.100\t61848\t205.204.101.182\t80\ttcp\t-\t106.362057 10.55.100.111\t57461\t172.217.8.198\t443\ttcp\tssl\t107.272461 10.55.100.111\t57459\t172.217.8.198\t443\ttcp\tssl\t107.268048 10.55.100.111\t57448\t23.201.85.182\t443\ttcp\tssl\t108.628962 10.55.100.106\t60207\t65.52.108.188\t443\ttcp\tssl\t2340.397545 10.55.100.111\t57477\t63.140.32.190\t443\ttcp\tssl\t105.333464 192.168.88.2\t123\t216.229.4.69\t123\tudp\tntp\t0.048235  Next, let\u0026rsquo;s introduce the sort command. * -n will sort based on numeric order * -r will reverse the sort so that the largest numbers are at the top * -k 7 tells sort to use the 7th column, which is our duration column\ncat conn.log | zeek-cut id.orig_h id.orig_p id.resp_h id.resp_p proto service duration | sort -nrk 7 | head  10.55.100.100\t49778\t65.52.108.225\t443\ttcp\t-\t86222.365445 10.55.100.107\t56099\t111.221.29.113\t443\ttcp\t-\t86220.126151 10.55.100.110\t60168\t40.77.229.82\t443\ttcp\t-\t86160.119664 10.55.100.109\t53932\t65.52.108.233\t443\ttcp\tssl\t72176.131072 10.55.100.105\t60214\t65.52.108.195\t443\ttcp\tssl\t66599.002312 10.55.100.103\t49918\t131.253.34.243\t443\ttcp\t-\t64698.370547 10.55.100.104\t63530\t131.253.34.246\t443\ttcp\tssl\t57413.278323 10.55.100.111\t63029\t111.221.29.114\t443\ttcp\t-\t46638.510373 10.55.100.108\t52989\t65.52.108.220\t443\ttcp\t-\t44615.165823 10.55.100.106\t52918\t40.77.229.91\t443\ttcp\tssl\t41206.913035  We are piping the contents of conn.log into zeek-cut which is reducing the number of fields to only show the ones we care about. Next we are using sort to sort the lines by the duration. And finally we are using head to only show us the top 10 entries.\nFrom here you can see which TCP connections were held open for the longest. In the output above we have a connection from 10.55.100.100 to 65.52.108.255 on port 443 that was open for 86,222 seconds, or 23.95 hours.\nNote that the conn.log shows connections for TCP, UDP, and ICMP all in the same file. So the above command is giving us the longest connections across all these protocols. If we wanted to filter out and see only the longest UDP connection, we use grep before the sort.\ncat conn.log | zeek-cut id.orig_h id.orig_p id.resp_h id.resp_p proto service duration | grep udp | sort -nrk 7 | head  10.55.182.100\t63546\t96.45.33.73\t8888\tudp\t-\t99.087045 10.55.182.100\t59685\t172.217.8.196\t443\tudp\t-\t59.539003 10.55.182.100\t63546\t96.45.33.73\t8888\tudp\t-\t59.322097 10.55.182.100\t63546\t96.45.33.73\t8888\tudp\t-\t56.383127 10.55.182.100\t63546\t96.45.33.73\t8888\tudp\t-\t55.771569 10.55.182.100\t63546\t96.45.33.73\t8888\tudp\t-\t54.537906 10.55.182.100\t59690\t172.217.4.34\t443\tudp\t-\t53.620681 10.55.182.100\t63546\t96.45.33.73\t8888\tudp\t-\t51.529226 10.55.182.100\t63546\t96.45.33.73\t8888\tudp\t-\t51.355111 10.55.182.100\t63546\t96.45.33.73\t8888\tudp\t-\t48.440549  Here we can see that the UDP long connection results from Zeek differ from what we get if we use Wireshark. This is because Zeek uses a 1 minute inactivity timeout when analyzing a UDP connection, while Wireshark did not use a timeout.\nCummulative Long Connections In some cases, malware might exhibit behavior that is somewhere between a single long connection, or a short, frequent beacon interval. Instead, it might open a connection, hold it open for a period of time (e.g. 30 minutes), and then immediately open another connection, and repeat.\nThe image below illustrates the difference by showing one single connection held open for 24 hours on top compared to 6 individual connections spread out over the 24 hours.\nWith this, an attacker is still able to maintain persistent communication for a long period of time but might not show up in the type of long connection analysis we\u0026rsquo;ve done so far. The following command will print the cummulative connection time from one IP address to the same destination IP and port.\ncat conn.log | zeek-cut id.orig_h id.resp_h id.resp_p proto duration | awk 'BEGIN{ FS=\u0026quot;\\t\u0026quot; } { arr[$1 FS $2 FS $3 FS $4] += $5 } END{ for (key in arr) printf \u0026quot;%s%s%s\\n\u0026quot;, key, FS, arr[key] }' | sort -nrk 5 | head  10.55.100.100\t65.52.108.225\t443\ttcp\t86222.4 10.55.100.107\t111.221.29.113\t443\ttcp\t86220.1 10.55.100.110\t40.77.229.82\t443\ttcp\t86160.1 10.55.100.109\t65.52.108.233\t443\ttcp\t72176.1 10.55.100.105\t65.52.108.195\t443\ttcp\t66599 10.55.100.103\t131.253.34.243\t443\ttcp\t64698.4 10.55.100.104\t131.253.34.246\t443\ttcp\t57413.3 10.55.100.111\t172.217.8.198\t443\ttcp\t55728.1 10.55.100.111\t111.221.29.114\t443\ttcp\t46658.4 10.55.100.108\t65.52.108.220\t443\ttcp\t44615.2  awk is a powerful scripting tool. However, the syntax starts to get very messy all on one line like this.\n BEGIN{ FS=\u0026quot;\\t\u0026quot; } - Set the FS (field separator) variable to a tab character. This is what is separating columns in our Zeek logs as well as what we want to use in our output. BEGIN means this instruction is only executed one time, before any data is processed. { arr[$1 FS $2 FS $3 FS $4] += $5 } - Creates an array (named arr) and adds up the duration ($5 is the fifth field, which is our duration). The important part here is that we are using the concatenation of the first four fields ($1 through $4) as our array key. Which means that as long as the source and destination IPs, destination port, and protocol remain the same it will add the duration to the total. awk executes this instruction repeatedly for every line of data. END{ for (key in arr) printf \u0026quot;%s%s%s\\n\u0026quot;, key, FS, arr[key] } - Here we are looping through all the elements in the array and printing out the results. END signifies that awk only executes this instruction one time, after processing all the data.  The results we get in this dataset are very similar to the results of overall long connections from the previous section. However, notice that previously there was no long connection from 10.55.100.111 to 172.217.8.198 in our top long connections. But here we can see that there were enough individual connections to bring it into our top 10.\nLet\u0026rsquo;s modify our command to:\n Disregard the destination port and protocol. If two IPs are directly communicating at all, no matter how, we want to know. This will catch cases where malware is switching between multiple ports or protocols on the same IP. Print out the number of connections that contributed to the overall duration. This will help us distinguish entries where there was a single long connection from those which had multiple connections.  cat conn.log | zeek-cut id.orig_h id.resp_h duration | awk 'BEGIN{ FS=\u0026quot;\\t\u0026quot; } { arr[$1 FS $2] += $3; count[$1 FS $2] += 1 } END{ for (key in arr) printf \u0026quot;%s%s%s%s%s\\n\u0026quot;, key, FS, count[key], FS, arr[key] }' | sort -nrk 4 | head  10.55.100.100\t65.52.108.225\t1\t86222.4 10.55.100.107\t111.221.29.113\t1\t86220.1 10.55.100.110\t40.77.229.82\t1\t86160.1 10.55.100.109\t65.52.108.233\t1\t72176.1 10.55.100.105\t65.52.108.195\t1\t66599 10.55.100.103\t131.253.34.243\t1\t64698.4 10.55.100.104\t131.253.34.246\t1\t57413.3 10.55.100.111\t172.217.8.198\t543\t56057.3 10.55.100.111\t111.221.29.114\t2\t46658.4 10.55.100.108\t65.52.108.220\t1\t44615.2  We can see that the results didn\u0026rsquo;t change even when we disregarded protocols and ports. However, now we can see that that pair from 10.55.100.111 to 172.217.8.198 had 543 separate connections where the rest of the list had 1 or 2. By dividing 56057 seconds by 543 connections we can see that each connection was open for an average of 103 seconds. If we wanted to investigate further, we could look at these individual connections\u0026rsquo; timing and duration or even perform some simple statistical analysis like we do in Beacons to see if there is any regularity or patterns in these connections.\nRITA RITA uses Zeek logs and should give us the same results as looking at the log files directly as we did above. If you haven\u0026rsquo;t already, import your log files as described in the Basic Tool Usage document.\nThe dataset name in this example is \u0026ldquo;sample\u0026rdquo;.\nrita show-long-connections -H --limit 10 sample  +---------------+----------------+--------------------------+----------+ | SOURCE IP | DESTINATION IP | DSTPORT:PROTOCOL:SERVICE | DURATION | +---------------+----------------+--------------------------+----------+ | 10.55.100.100 | 65.52.108.225 | 443:tcp:- | 86222.4s | | 10.55.100.107 | 111.221.29.113 | 443:tcp:- | 86220.1s | | 10.55.100.110 | 40.77.229.82 | 443:tcp:- | 86160.1s | | 10.55.100.109 | 65.52.108.233 | 443:tcp:ssl | 72176.1s | | 10.55.100.105 | 65.52.108.195 | 443:tcp:ssl | 66599s | | 10.55.100.103 | 131.253.34.243 | 443:tcp:- | 64698.4s | | 10.55.100.104 | 131.253.34.246 | 443:tcp:ssl | 57413.3s | | 10.55.100.111 | 111.221.29.114 | 443:tcp:- | 46638.5s | | 10.55.100.108 | 65.52.108.220 | 443:tcp:- | 44615.2s | | 10.55.100.106 | 40.77.229.91 | 443:tcp:ssl | 41206.9s | +---------------+----------------+--------------------------+----------+  Note that RITA will lump together TCP, UDP, and ICMP connections as they are in Zeek\u0026rsquo;s conn.log and there is no way to separate out results by protocol using RITA.\n"},{"uri":"/beacons/","title":"Beacons","tags":[],"description":"","content":" Goal Identify command \u0026amp; control (C2) sessions using regular connections between two IP addresses.\nTools Used  Wireshark Tshark Spreadsheet program (e.g. LibreOffice Calc) RITA  Hunt We\u0026rsquo;re going to find the connections that happen on regular time intervals. These could indicate an attacker with a persistent C2 session.\nDuring this lab, we will be analyzing traffic between two particular systems: 192.168.88.2 and 165.227.88.15. You\u0026rsquo;d use the techniques in this lab after identifying systems with suspiscious traffic between then, such as systems with a large number of connections or data sent.\nWireshark Open your pcap in Wireshark. This loads every individual packet in the main window.\nApply the filter ip.src==192.168.88.2 \u0026amp;\u0026amp; ip.dst==165.227.88.15. This exercise is meant to illustrate the process that is automated with tools such as RITA. Your window should now look something like this:\nIf you look at the Time column, you can already see a pattern emerge. The seconds increase by 1 pretty consistently.\nOpen the Column Preferences by right-clicking on one of the columns (e.g. Destination) and selecting Column Preferences from the menu.\nInside the preferences, click the + button to add a new item and make it match the entry shown below with \u0026ldquo;Time Delta\u0026rdquo; for the title and \u0026ldquo;frame.time_delta_displayed\u0026rdquo; in the Fields column. You need to double-click inside the box to edit the values. Then uncheck the Info/Information line to temporarily hide that item and click OK.\nYou should then see a new column that contains the difference in times between each packet and the previous one.\nOur previous hunch that most packets are 1 second apart is confirmed. This shows that the IP 192.168.88.2 was communicating with 165.227.88.15 consistently on a 1 second interval.\nTshark Tshark is the command line equivalent of Wireshark. We can use it to process pcaps and pull out different fields using its protocol dissectors. This is useful if we want to use other tools to manipulate the data.\ntshark -r sample.pcap -T fields -e ip.src -e ip.dst -e udp.dstport -e frame.time_delta_displayed 'ip.src==192.168.88.2 \u0026amp;\u0026amp; ip.dst==165.227.88.15' | head -25  192.168.88.2\t165.227.88.15\t53\t0.000000000 192.168.88.2\t165.227.88.15\t53\t1.074819358 192.168.88.2\t165.227.88.15\t53\t1.084471967 192.168.88.2\t165.227.88.15\t53\t1.078728781 192.168.88.2\t165.227.88.15\t53\t1.069749570 192.168.88.2\t165.227.88.15\t53\t1.077714934 192.168.88.2\t165.227.88.15\t53\t1.076642909 192.168.88.2\t165.227.88.15\t53\t1.070790122 192.168.88.2\t165.227.88.15\t53\t1.071048506 192.168.88.2\t165.227.88.15\t53\t1.064914560 192.168.88.2\t165.227.88.15\t53\t0.093778795 192.168.88.2\t165.227.88.15\t53\t0.961346162 192.168.88.2\t165.227.88.15\t53\t1.062188142 192.168.88.2\t165.227.88.15\t53\t1.065854491 192.168.88.2\t165.227.88.15\t53\t1.075033821 192.168.88.2\t165.227.88.15\t53\t1.066068845 192.168.88.2\t165.227.88.15\t53\t1.063321512 192.168.88.2\t165.227.88.15\t53\t1.071506357 192.168.88.2\t165.227.88.15\t53\t1.058017495 192.168.88.2\t165.227.88.15\t53\t1.075381485 192.168.88.2\t165.227.88.15\t53\t1.078740794 192.168.88.2\t165.227.88.15\t53\t1.062031167 192.168.88.2\t165.227.88.15\t53\t1.065914897 192.168.88.2\t165.227.88.15\t53\t1.063939728 192.168.88.2\t165.227.88.15\t53\t1.066905553  The tshark arguments are:\n -r sample.pcap - The path to your pcap file. -T fields - Tell tshark to output values of the specified fields. -e ip.src -e ip.dst -e udp.dstport -e frame.time_delta_displayed - These options are telling tshark which fields should be printed. In this case, we want the source and destination IP, the destination port (you could also try tcp.dstport) and the time since the previous packet was sent. This syntax is the same as used in Wireshark. You can find a list of all fields in Wiresharks\u0026rsquo; documentation. ip.src==192.168.88.2 \u0026amp;\u0026amp; ip.dst==165.227.88.15 - The filter to be used. This uses Wireshark\u0026rsquo;s display filter syntax. In this case we are telling tshark to only process packets sent from 192.168.88.2 to 165.227.88.15.  For instance, you could use the following command to output all the packet sizes and the time intervals to a CSV file.\ntshark -r sample.pcap -T fields -E separator=, -e ip.len -e frame.time_delta_displayed 'ip.src==192.168.88.2 \u0026amp;\u0026amp; ip.dst==165.227.88.15' \u0026gt; sample.csv  And then open the file in a spreadsheet program and calculate some basic statistics, such as the min, max, mean, and standard deviation of the data.\nHint: Here are the formulas used in the above example. But you can have them automatically generated in LibreOffice Calc by using Data -\u0026gt; Statistics -\u0026gt; Descriptive Statistics menu item.\nRITA RITA calculates various statistical measures for both the time interval between connections and the size of each connection.\nIf you haven\u0026rsquo;t already, generate Zeek logs from the pcap, and import the Zeek files as described in the Basic Tool Usage document.\nThe dataset name in this example is \u0026ldquo;sample\u0026rdquo;.\nrita show-beacons sample | head  Score,Source IP,Destination IP,Connections,Avg Bytes,Intvl Range,Size Range,Top Intvl,Top Size,Top Intvl Count,Top Size Count,Intvl Skew,Size Skew,Intvl Dispersion,Size Dispersion 1,192.168.88.2,165.227.88.15,108858,199,860,230,1,89,53341,108319,0,0,0,0 1,10.55.100.111,165.227.216.194,20054,92,29,52,1,52,7774,20053,0,0,0,0 0.838,10.55.200.10,205.251.194.64,210,308,29398,4,300,70,109,205,0,0,0,0 0.835,10.55.200.11,205.251.197.77,69,308,1197,4,300,70,38,68,0,0,0,0 0.834,192.168.88.2,13.107.5.2,27,198,2,33,12601,73,4,15,0,0,0,0 0.834,10.55.100.111,34.239.169.214,34,704,5,4517,1,156,15,30,0,0,0,0 0.833,10.55.182.100,23.52.161.212,25,777,41422,40,1800,465,19,13,0,0,0,0 0.833,10.55.100.108,23.52.161.212,24,1183,43303,0,1800,505,15,24,0,0,0,0 0.833,10.55.100.108,23.52.162.184,24,2232,43303,0,1800,467,18,24,0,0,0,0  In this case, the ascii table output is not as useful as it is too wide. However, you can pipe the output to a new CSV file and open it in a spreadsheet program. This will give you the flexibility to filter and sort based on different columns.\nrita show-beacons sample \u0026gt; sample.csv  Or you can export the results to an HTML report and view the data in a web browser.\nThe different columns are as follows:\n Score - The score is a metric calculated by taking into account the interval skew, dispersion, and duration as well as the data size skew, dispersion, and mode. The closer this score is to 1 the more likely that this is beaconing activity. Source - IP address that initiated the connections. Destination - IP address that received the connections. Connections - The total number of connections between the source and destination IPs. Avg Bytes - The average number of bytes transferred in either direction per connection. Intvl Range - This is the difference between the maximum and minimum time intervals seen. For instance, if there were 2 connections 80 seconds apart and another 2 connections 20 seconds apart then the entire range of intervals would be 80-20 = 60 seconds. This is useful to know how spread out or close together the connections are, but it is easily thrown off by missed connections. Size Range - This is the difference between the maximum and minimum connection sizes seen. Top Intvl (CSV) / Intvl Mode (HTML) - The interval between connections that we saw the most. Top Size (CSV) / Size Mode (HTML) - The number of bytes transferred in a connection that we saw the most. Top Intvl Count (CSV) / Intvl Mode Count (HTML) - How many times we saw the interval mode. Top Size Count (CSV) / Size Mode Count (HTML) - How many times we saw the size mode. Intvl Skew - Skew measures how distorted or asymmetric the data is. A value closer to 0 means that the data is very symmetric. This measure is useful in the case of malware that does not try to hide itself, but more importantly will detect malware that tries to hide by adding jitter. This works because malware with jitter most likely uses a random number generator to add or subtract to a mean value (e.g. 30 seconds +/- 5 seconds). The random number generator will uniformly distribute the values which causes the data to be symmetric, and as such this particular measure is hard to beat. Size Skew - Skew of the connection data sizes. Skew isn\u0026rsquo;t as important of a measure for data sizes because we expect them to naturally vary if an attacker is actively sending commands or transferring data via a C2 session. But the majority of the time we expect the C2 session to be simply checking in. Intvl Dispersion / Size Dispersion - Dispersion describes how likely it is that an interval or data size is to stray from the mean (very similar to standard deviation from the mean). A value closer to 0 means that most intervals or data sizes were clustered around the mean and had very little variation. This is useful in the case of beacons that don\u0026rsquo;t make any effort to hide themselves by changing their beacon interval or data sizes. The more jitter added to a beacon, the less effective dispersion is.  "},{"uri":"/dns/","title":"DNS","tags":[],"description":"","content":" Goal Leverage frequency analysis to identify systems using DNS for C2.\nTools Used  dig Zeek Tshark Wireshark RITA  Background Many command \u0026amp; control (C2) channels communicate directly with an attacker-controlled system. This makes it easier to detect and track down. DNS based C2 is different as the communication utilizes the DNS infrastructure to communicate instead.\nA normal DNS request for google.com goes like this:\nNon-Cached DNS Query\nsequenceDiagram Client-\u0026gt;\u0026gt;Recursive NS: A for google.com? Recursive NS-\u0026gt;\u0026gt;Root NS: NS for com? Root NS-\u0026gt;\u0026gt;Recursive NS: a.gtld-servers.net (192.5.6.30) Recursive NS-\u0026gt;\u0026gt;.com NS: NS for google.com? .com NS-\u0026gt;\u0026gt;Recursive NS: ns1.google.com (216.239.34.10) Recursive NS-\u0026gt;\u0026gt;Google NS: A for google.com? Google NS-\u0026gt;\u0026gt;Recursive NS: 172.217.1.46 Recursive NS-\u0026gt;\u0026gt;Client: 172.217.1.46  Everything going on might not yet be clear, but there are two things to notice first.\n The Client (your computer) only talks with its configured Recursive Name Server (NS). This name server is usually given to your computer when it connects to a network through DHCP. Though you can also set your DNS servers manually. For instance, you may choose to use one of the servers provided by large companies such as Cloudflare (1.1.1.1) or Google (8.8.8.8).\n Behind the scenes, your Recursive NS is doing a lot of work for you. All these requests would quickly overwhelm servers if they were done every time so the above sequence of events only happens if your Recursive NS doesn\u0026rsquo;t already know an answer. Once it has performed this work once it will cache the answer for a period of time. So the next time you or any other client makes the same DNS query the NS will answer from its local cache instead of querying other name servers.\n  Cached DNS Query\nsequenceDiagram Client-\u0026gt;\u0026gt;Recursive NS: A for google.com? Note right of Recursive NS: answer in cache Recursive NS-\u0026gt;\u0026gt;Client: 172.217.1.46  These two points are both very important when considering DNS as a C2 channel.\nFirst, the Recursive NS is effectively acting to proxy traffic between the client and the remote name servers. Since DNS is so critical to normal network operations most networks will implicitly trust whichever recursive NS is configured with DHCP. Attackers exploit this trust to communicate out of restrictive networks.\nSecond, since a name server will cache results for subsequent requests attackers need to prevent caching in order to get the NS to communicate out with their C2 server. A name server will use its cache if it has answered an identical request recently. To get around this, attackers ensure that they never make identical requests. In practice, this means the more requests an attacker sends out the more unique subdomains they need to use. Defenders can use this behavior to detect C2 traffic over DNS.\nExercise: Follow the Sequence of a DNS Query Let\u0026rsquo;s go through the above sequence diagram and understand each step that is happening. We will use the dig command to simulate what is going on behind the scenes.\nNote: You may receive different IP addresses when you run these same commands. This is because large websites like Google will host their content on many different servers. They use the DNS results to both direct you to the closest geographical server and load balance so that no one server gets overloaded.\n The client asks the recursive name server for google.com\u0026rsquo;s A record.  We\u0026rsquo;ll make heavy use of dig\u0026rsquo;s +norecurse option to prevent the background requests that name servers will typically make on your behalf.\nIt\u0026rsquo;s highly likely that your name server of choice already has an answer cached for google.com. You may have to try a few different domains before you find one that isn\u0026rsquo;t cached. Here\u0026rsquo;s an example using do-not-respond.org.\ndig do-not-respond.org A +norecurse  ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: SERVFAIL, id: 51547 ;; flags: qr ra; QUERY: 1, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1452 ;; QUESTION SECTION: ;do-not-respond.org.\tIN\tA  You can see that there was no answer provided and the header shows Answer: 0.\nTry the query again without the +norecurse flag.\ndig do-not-respond.org A  ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 43574 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1452 ;; QUESTION SECTION: ;do-not-respond.org.\tIN\tA ;; ANSWER SECTION: do-not-respond.org.\t7200\tIN\tA\t99.81.40.78  This time an answer was provided because your local resolver made the recursive calls for you. If we try the first command again with the +norecurse flag we\u0026rsquo;ll see that the answer is now cached.\ndig do-not-respond.org A +norecurse  ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 14157 ;; flags: qr ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1452 ;; QUESTION SECTION: ;do-not-respond.org.\tIN\tA ;; ANSWER SECTION: do-not-respond.org.\t7198\tIN\tA\t99.81.40.78  Let\u0026rsquo;s explore what happens from the recursive name server\u0026rsquo;s point of view if the result is not in the cache.\n The recursive name server asks the root name server for the com NS record.  DNS infrastructure is set up in a tree hierarchy with 13 root name servers on the top. The IP addresses of the root name servers are well known and rarely change. The recursive name server will have those already so that it knows where to send its next request.\nThe root name server will consult its zone file and return the NS record(s) for the com top level domain (TLD). The zone file can be found here: https://www.internic.net/zones/root.zone\nIt contains NS records for each of the TLDs (e.g. com, net, org, shop, fitness, etc.) and the A \u0026amp; AAAA records for each of the NS records. This is so that recursive resolvers will know the IP address(es) of the next name servers to query. Download the root zone file and see if you can find the entries for com\u0026rsquo;s name servers that dig shows us below.\n@198.41.0.4 tells dig to use that IP address as the name server to query, which is one of the root name servers. We\u0026rsquo;re asking the name server for the NS record for com.\ndig @198.41.0.4 com NS +norecurse  ;; QUESTION SECTION: ;com.\tIN\tNS ;; AUTHORITY SECTION: com.\t172800\tIN\tNS\te.gtld-servers.net. com.\t172800\tIN\tNS\tb.gtld-servers.net. com.\t172800\tIN\tNS\tj.gtld-servers.net. com.\t172800\tIN\tNS\tm.gtld-servers.net. com.\t172800\tIN\tNS\ti.gtld-servers.net. com.\t172800\tIN\tNS\tf.gtld-servers.net. com.\t172800\tIN\tNS\ta.gtld-servers.net. com.\t172800\tIN\tNS\tg.gtld-servers.net. com.\t172800\tIN\tNS\th.gtld-servers.net. com.\t172800\tIN\tNS\tl.gtld-servers.net. com.\t172800\tIN\tNS\tk.gtld-servers.net. com.\t172800\tIN\tNS\tc.gtld-servers.net. com.\t172800\tIN\tNS\td.gtld-servers.net. ;; ADDITIONAL SECTION: e.gtld-servers.net.\t172800\tIN\tA\t192.12.94.30 e.gtld-servers.net.\t172800\tIN\tAAAA\t2001:502:1ca1::30 b.gtld-servers.net.\t172800\tIN\tA\t192.33.14.30 b.gtld-servers.net.\t172800\tIN\tAAAA\t2001:503:231d::2:30 j.gtld-servers.net.\t172800\tIN\tA\t192.48.79.30 j.gtld-servers.net.\t172800\tIN\tAAAA\t2001:502:7094::30 m.gtld-servers.net.\t172800\tIN\tA\t192.55.83.30 m.gtld-servers.net.\t172800\tIN\tAAAA\t2001:501:b1f9::30 i.gtld-servers.net.\t172800\tIN\tA\t192.43.172.30 i.gtld-servers.net.\t172800\tIN\tAAAA\t2001:503:39c1::30 f.gtld-servers.net.\t172800\tIN\tA\t192.35.51.30 f.gtld-servers.net.\t172800\tIN\tAAAA\t2001:503:d414::30 a.gtld-servers.net.\t172800\tIN\tA\t192.5.6.30 a.gtld-servers.net.\t172800\tIN\tAAAA\t2001:503:a83e::2:30 g.gtld-servers.net.\t172800\tIN\tA\t192.42.93.30 g.gtld-servers.net.\t172800\tIN\tAAAA\t2001:503:eea3::30 h.gtld-servers.net.\t172800\tIN\tA\t192.54.112.30 h.gtld-servers.net.\t172800\tIN\tAAAA\t2001:502:8cc::30 l.gtld-servers.net.\t172800\tIN\tA\t192.41.162.30 l.gtld-servers.net.\t172800\tIN\tAAAA\t2001:500:d937::30 k.gtld-servers.net.\t172800\tIN\tA\t192.52.178.30 k.gtld-servers.net.\t172800\tIN\tAAAA\t2001:503:d2d::30 c.gtld-servers.net.\t172800\tIN\tA\t192.26.92.30 c.gtld-servers.net.\t172800\tIN\tAAAA\t2001:503:83eb::30 d.gtld-servers.net.\t172800\tIN\tA\t192.31.80.30 d.gtld-servers.net.\t172800\tIN\tAAAA\t2001:500:856e::30   The recursive name server asks com\u0026rsquo;s authoritative name server for google.com\u0026rsquo;s NS record.  Next, we\u0026rsquo;ll take the IP for one of the returned com NS records, a.gtld-servers.net: 192.5.6.30 and ask it which name server to use for google.com.\ndig @192.5.6.30 google.com NS +norecurse  ;; QUESTION SECTION: ;google.com.\tIN\tNS ;; AUTHORITY SECTION: google.com.\t172800\tIN\tNS\tns2.google.com. google.com.\t172800\tIN\tNS\tns1.google.com. google.com.\t172800\tIN\tNS\tns3.google.com. google.com.\t172800\tIN\tNS\tns4.google.com. ;; ADDITIONAL SECTION: ns2.google.com.\t172800\tIN\tAAAA\t2001:4860:4802:34::a ns2.google.com.\t172800\tIN\tA\t216.239.34.10 ns1.google.com.\t172800\tIN\tAAAA\t2001:4860:4802:32::a ns1.google.com.\t172800\tIN\tA\t216.239.32.10 ns3.google.com.\t172800\tIN\tAAAA\t2001:4860:4802:36::a ns3.google.com.\t172800\tIN\tA\t216.239.36.10 ns4.google.com.\t172800\tIN\tAAAA\t2001:4860:4802:38::a ns4.google.com.\t172800\tIN\tA\t216.239.38.10   The recursive name server asks google.com\u0026rsquo;s name server for google.com\u0026rsquo;s A record.  Finally, we can use the name server returned to ask for the IP address of google.com.\ndig @216.239.32.10 google.com A +norecurse  ;; QUESTION SECTION: ;google.com.\tIN\tA ;; ANSWER SECTION: google.com.\t300\tIN\tA\t172.217.1.46  You can then put 172.217.1.46 in your web browser and Google\u0026rsquo;s homepage will load.\nHunt We\u0026rsquo;re going to exploit the fact that C2 over DNS needs to defeat name server caches by using unique subdomains.\nZeek Subdomain Frequency Analysis Be sure to analyze your pcap using Zeek before starting.\nYour Zeek logs should include a file called dns.log. You can inspect what\u0026rsquo;s in this file using the head command.\nhead dns.log  #separator \\x09 #set_separator\t, #empty_field\t(empty) #unset_field\t- #path\tdns #open\t2019-10-16-15-13-09 #fields\tts\tuid\tid.orig_h\tid.orig_p\tid.resp_h\tid.resp_p\tproto\ttrans_id\trtt\tquery\tqclass\tqclass_name\tqtype\tqtype_name\trcode\trcode_name\tAA\tTC\tRD\tRA\tZ\tanswers\tTTLs\trejected #types\ttime\tstring\taddr\tport\taddr\tport\tenum\tcount\tinterval\tstring\tcount\tstring\tcount\tstring\tcount\tstring\tbool\tbool\tbool\tbool\tcount\tvector[string]\tvector[interval]\tbool 1517336042.279652\tComPBK1vso3uDC8KS2\t192.168.88.2\t55638\t165.227.88.15\t53\tudp\t42937\t0.069982\t6dde0175375169c68f.dnsc.r-1x.com\t1\tC_INTERNET\t16\tTXT\t0NOERROR\tF\tF\tT\tT\t0\tTXT 18 302f017537c68f5169\t60.000000\tF 1517336043.354471\tCyZ4x32ytwoKUgqozf\t192.168.88.2\t28736\t165.227.88.15\t53\tudp\t16556\t0.078779\t0b320175375169c68f.dnsc.r-1x.com\t1\tC_INTERNET\t16\tTXT\t0NOERROR\tF\tF\tT\tT\t0\tTXT 18 c27a017537c68f5169\t60.000000\tF  This isn\u0026rsquo;t very readable on it\u0026rsquo;s own. There are too many columns to display on a single line. Let\u0026rsquo;s use zeek-cut to reduce the columns to what we\u0026rsquo;d like to look at.\nhead dns.log | zeek-cut -c id.orig_h query qtype_name answers  #separator \\x09 #set_separator\t, #empty_field\t(empty) #unset_field\t- #path\tdns #open\t2019-10-16-15-13-09 #fields\tid.orig_h\tquery\tqtype_name\tanswers #types\taddr\tstring\tstring\tvector[string] 192.168.88.2\t6dde0175375169c68f.dnsc.r-1x.com\tTXT\tTXT 18 302f017537c68f5169 192.168.88.2\t0b320175375169c68f.dnsc.r-1x.com\tTXT\tTXT 18 c27a017537c68f5169  This is better, but so far we are only processing the first few lines of the file. Using cat instead of head will show the entire file scroll by.\ncat dns.log | zeek-cut -c id.orig_h query qtype_name answers  ... 192.168.88.2\t5fd2011239458783cf.dnsc.r-1x.com\tTXT\tTXT 18 054101123983cf4587 10.55.200.10\ta849.dscg2.akamai.net\tA\t- 10.55.200.10\ttarget-opus.map.fastly.net\tA\t- 10.55.200.10\taccdn.lpsnmedia.net\tA\t- 10.55.200.10\tdc.ads.linkedin.com\tA\t- 10.55.200.10\tus-scproxy.alibaba.com.gds.alibabadns.com\tA\t- 10.55.200.10\tva.v.liveperson.net\tA\t- 192.168.88.2\t36a80176d2f1ce66e2.dnsc.r-1x.com\tTXT\tTXT 18 77210176d266e2f1ce 10.55.200.10\twww.target.com.edgekey.net\tA\t- 10.55.200.10\te10583.dspg.akamaiedge.net\tA\t- 192.168.88.2\t66b00176d2f1ce66e2.dnsc.r-1x.com\tTXT\tTXT 18 f95e0176d266e2f1ce 192.168.88.2\t7d0e0176d2f1ce66e2.dnsc.r-1x.com\tTXT\tTXT 18 7cb50176d266e2f1ce 10.55.200.10\tlptag.liveperson.cotcdb.net.livepersonk.akadns.net\tA\t- 192.168.88.2\t3573011239458783cf.dnsc.r-1x.com\tTXT\tTXT 18 983c01123983cf4587 ...  Just glancing at the data scrolling past you may notice some odd looking queries that you want to investigate. First, let\u0026rsquo;s see if we can summarize our data a little bit better. The following command counts the number of unique subdomains for each base domains.\ncat dns.log | zeek-cut query | sort | uniq | rev | cut -d '.' -f 1-2 | rev | sort | uniq -c | sort -nr | head   62468 r-1x.com 154 akamaiedge.net 125 akadns.net 121 edgekey.net 101 amazonaws.com 67 microsoft.com 51 dynect.net 45 parsely.com 44 akam.net 43 cloudfront.net  Here is a breakdown of the above command:\n cat dns.log | zeek-cut query - Ignore everything but the query field, which tells us what domain was requested. sort | uniq - Remove all duplicate queries. rev - Takes each query and reverses the string, so that www.google.com becomes moc.elgoog.www. The reason we do this is to strip the query down to the top level domain (TLD), like .com or .net, and the next portion of the domain. cut -d '.' -f 1-2 - Split the full query on every period and keep the first and second elements (e.g moc.elgoog.www -\u0026gt; moc.elgoog). rev - Reverse the string again to bring it back to normal. sort | uniq -c - Remove and count duplicates. sort -nr | head - Output the entries with the most duplicates.  You are encouraged to play around with the form of this command to both understand it better and see what else you can get from the data. For instance, you could delete each of the commands from the end and see what happens. Or you could change the number 2 in the cut -d '.' -f 1-2 command. See if you can understand the purpose of each of the steps on a deeper level.\nLet\u0026rsquo;s think about what we\u0026rsquo;ve just done and how it applies to threat hunting. We\u0026rsquo;ve removed all duplicate DNS queries, meaning that every query processed was for a unique domain. Next, we stripped down every long domain name to just its base domain. Then we counted the duplicate entries for each of those base domains. In the end, what we\u0026rsquo;ve done is count the number of subdomains for each of the base domains and displayed the domains with the largest number of subdomains.\nAfter taking into consideration what we\u0026rsquo;ve learned in the Background section, it should be apparent that something fishy is going on with the domain r-1x.com. Our next steps could be:\n Look at a few samples of queries for this domain  cat dns.log | zeek-cut id.orig_h query answers | grep 'r-1x\\.com'  ... 192.168.88.2\t6dde0175375169c68f.dnsc.r-1x.com\tTXT 18 302f017537c68f5169 192.168.88.2\t0b320175375169c68f.dnsc.r-1x.com\tTXT 18 c27a017537c68f5169 192.168.88.2\t344b0175375169c68f.dnsc.r-1x.com\tTXT 18 67f2017537c68f5169 192.168.88.2\t0f370175375169c68f.dnsc.r-1x.com\tTXT 18 8759017537c68f5169 192.168.88.2\t251e0175375169c68f.dnsc.r-1x.com\tTXT 18 cf5d017537c68f5169 192.168.88.2\t31610175375169c68f.dnsc.r-1x.com\tTXT 18 4a42017537c68f5169 192.168.88.2\t1a600175375169c68f.dnsc.r-1x.com\tTXT 18 50fa017537c68f5169 192.168.88.2\t69a60175375169c68f.dnsc.r-1x.com\tTXT 18 a7be017537c68f5169 192.168.88.2\t06540175375169c68f.dnsc.r-1x.com\tTXT 18 6d55017537c68f5169 192.168.88.2\t4bff0175375169c68f.dnsc.r-1x.com\tTXT 18 414c017537c68f5169  Right away, these queries don\u0026rsquo;t look like what we\u0026rsquo;re used to. The queries mostly have a strange looking hexadecimal string followed by .dnsc.r-1x.com. The queries are mostly, if not all, TXT type and the responses are also mostly a fixed length hexadecimal string. Take a look through more of the results and see if you can spot other types of queries and answers and determine any patterns.\n Find all the IP addresses that performed queries for this domain  cat dns.log | zeek-cut id.orig_h query | grep 'r-1x\\.com' | cut -f 1 | sort | uniq -c   109227 192.168.88.2  Luckily in our case, all the queries to the identified suspicious domain have come from a single IP: 192.168.88.2. Not so luckily, this IP address happens to be our network\u0026rsquo;s local DNS forwarder, which means that all the queries actually originated from other IP(s) and to find out which ones we would have to consult our DNS server\u0026rsquo;s logs. This is important to note as the network capture point can affect the amount of information you have when threat hunting.\nThere is more data we can glean from the Zeek logs. This command is pulling out all the answers which have IP addresses in them. In this case, there is only one: 165.227.88.15.\ncat dns.log | zeek-cut query answers | grep 'r-1x\\.com' | cut -f 2 | cut -d ' ' -f 3 | egrep '([0-9]{0,3}\\.)[0-9]{0,3}' | sort | uniq  165.227.88.15  We can then use this IP to see if there were any internal systems making direct connections to this server.\ncat conn.log | zeek-cut id.orig_h id.resp_h id.resp_p proto service | grep '165.227.88.15' | sort | uniq -c   2 165.227.88.15\t192.168.88.2\t3\ticmp\t- 2 192.168.88.2\t165.227.88.15\t53\ttcp\t- 108856 192.168.88.2\t165.227.88.15\t53\tudp\tdns  Here we can see that there were three unique connection types involving the suspicious IP:\n 2 connections where 165.227.88.15 responded with icmp messages 2 connections where 192.168.88.2 connected to tcp/53 108,856 connections where 192.168.88.2 connected to udp/53  The next best course of action would be to consult the logs from our DNS server, if available, to find out which of our internal hosts are possibly compromised.\nQuery Type Analysis Another telltale sign of DNS C2 channels is an unusually high number of a certain query type. We\u0026rsquo;d expect normal DNS traffic to be mostly A / AAAA and CNAME types, with the rest being relatively uncommon.\nHere is an example taken from a dataset that does not have DNS based C2.\n 224390 A 8711 AAAA 446 PTR 271 DS 121 DNSKEY 65 TXT 10 SRV 10 SOA 6 - 2 NS  The other types of records can be more prevalent in certain scenarios. It is important to know whether or not to expect a certain type of requests so that you can identify if they are being misused for C2.\n PTR lookups are also common when a mail server, web server or network monitoring station is looking up the hostnames associated with connecting IP addresses. DNSKEY and DS records are used when DNSSEC is implemented or verified. TXT records are mainly used for domain ownership validation or combatting spam.  Here is how to generate a similar output for the sample in question. This is the number of unique queries for each query type.\ncat dns.log | zeek-cut qtype_name | sort | uniq -c | sort -nr   199818 A 108911 TXT 6751 AAAA 91 SRV 46 SOA 8 DNSKEY 7 DS 2 NS  Notice the abnormally large number of TXT queries. These types of queries allow attackers higher bandwidth than A records as each reply can contain more characters.\n Several DNS request types are supported, with the NULL and PRIVATE types expected to provide the largest downstream bandwidth. [\u0026hellip;] Other available types are TXT, SRV, MX, CNAME and A (returning CNAME), in decreasing bandwidth order.\nSource: https://github.com/yarrick/iodine#operational-info\n dnscat2, a popular open-source malware uses TXT, CNAME, \u0026amp; MX type queries by default, though that is dependent on the client implementation.\nTshark You can also pull out DNS queries straight from a pcap using tshark. The command below will count the number of unique subdomains per each base domain.\ntshark -r sample.pcap -T fields -e dns.qry.name udp.dstport==53 | sort | uniq | rev | cut -d '.' -f 1-2 | rev | sort | uniq -c | sort -nr | head -10   62468 r-1x.com 154 akamaiedge.net 125 akadns.net 121 edgekey.net 104 amazonaws.com 67 microsoft.com 51 dynect.net 45 parsely.com 44 akam.net 43 cloudfront.net  See the Zeek section above for an explanation on the entire command. The tshark arguments are explained here:\n -r sample.pcap - The path to your pcap file. -T fields - Tell tshark to output values of the specified fields. -e dns.qry.name - The field to print from every DNS packet. This syntax is the same as used in Wireshark. You can find a list of other DNS-related fields in Wiresharks\u0026rsquo; documentation. udp.dstport==53 - The filter to be used. This uses Wireshark\u0026rsquo;s display filter syntax. In this case we are telling tshark to only process packets sent to UDP port 53.  Wireshark Open your pcap in Wireshark. This loads every individual packet in the main window.\nUnder the Statistics menu select DNS. The DNS window analyzes and displays metrics about all the DNS queries in the pcap. We will use this window to see a query type analysis.\nSort by clicking on the Count column. Then click it again to sort in decreasing order.\nThe Query Type shows the number of queries by type. However, this does not remove duplicates queries.\nRITA RITA uses Zeek logs and should give us the same results as looking at the log files directly as we did above. If you haven\u0026rsquo;t already, import your log files as described in the Basic Tool Usage document.\nThe dataset name in this example is \u0026ldquo;sample\u0026rdquo;.\nrita show-exploded-dns -H --limit 10 sample  +-------------------+-------------------+-----------------+ | DOMAIN | UNIQUE SUBDOMAINS | TIMES LOOKED UP | +-------------------+-------------------+-----------------+ | r-1x.com | 62468 | 109227 | | dnsc.r-1x.com | 62466 | 108911 | | akamaiedge.net | 154 | 27381 | | akadns.net | 125 | 13907 | | edgekey.net | 121 | 7110 | | amazonaws.com | 101 | 13297 | | elb.amazonaws.com | 90 | 13259 | | com.edgekey.net | 88 | 6075 | | microsoft.com | 67 | 1687 | | com.akadns.net | 59 | 8405 | +-------------------+-------------------+-----------------+  The command above is nearly equivalent to the processing of Zeek logs above to count the number of subdomains per base domain. However, RITA counts the number of subdomains for every domain and not just the base domains. You can see that above since there are results for dnsc.r-1x.com and elb.amazonws.com.\n"},{"uri":"/outliers/","title":"Outliers","tags":[],"description":"","content":" Goal Identify systems with suspiciously high or low metrics in different areas. These outliers can then be used to dig into further with other types of analysis.\nTools Used  Zeek  Hunt All of the scenarios here use Zeek logs. So be sure to analyze your pcap using Zeek before starting.\nNumber of Connections More advanced techniques look at how long connections are held open or how regular connections are being made between two IP addresses. Sometimes it is useful to simply see how many total connections were made. A high number of connections made indicates that systems were communicating quite a bit and this type of analysis can be used to determine where to dig further.\ncat conn.log | zeek-cut id.orig_h id.resp_h id.resp_p proto | awk 'BEGIN{ FS=\u0026quot;\\t\u0026quot; } { arr[$1 FS $2 FS $3 FS $4] += 1 } END{ for (key in arr) printf \u0026quot;%s%s%s\\n\u0026quot;, key, FS, arr[key] }' | sort -nrk 5 | head  192.168.88.2\t165.227.88.15\t53\tudp\t108856 10.55.200.10\t172.16.200.11\t53\tudp\t64285 10.55.100.111\t165.227.216.194\t443\ttcp\t20054 10.55.182.100\t10.233.233.5\t80\ttcp\t4190 10.55.200.10\t216.239.34.10\t53\tudp\t3856 10.55.200.11\t193.108.88.128\t53\tudp\t3660 10.55.200.11\t88.221.81.192\t53\tudp\t2742 10.55.200.11\t205.251.195.166\t53\tudp\t2289 10.55.200.11\t216.239.34.10\t53\tudp\t2265 10.55.200.10\t205.251.195.166\t53\tudp\t1931   cat conn.log | zeek-cut id.orig_h id.resp_h id.resp_p proto - We\u0026rsquo;re taking Zeek\u0026rsquo;s conn.log and only keeping the source IP, destination IP, destination port, and destination protocol. awk - The following explains the pieces of the awk script.  BEGIN{ FS=\u0026quot;\\t\u0026quot; } - Set the FS (field separator) variable to a tab character. This is what is separating columns in our Zeek logs as well as what we want to use in our output. BEGIN means this instruction is only executed one time, before any data is processed. { arr[$1 FS $2 FS $3 FS $4] += 1 } - Creates an array (named arr) with the number of connections. The important part here is that we are using the concatenation of the first four fields ($1 through $4) as our array key. Which means that as long as the source and destination IPs, destination port, and protocol remain the same it counts the connections under the same key. awk executes this instruction repeatedly for every line of data. END{ for (key in arr) printf \u0026quot;%s%s%s\\n\u0026quot;, key, FS, arr[key] } - Here we are looping through all the elements in the array and printing out the results. END signifies that awk only executes this instruction one time, after processing all the data.  sort -nrk 5 | head - The number of connections is the 5th and final column printed in the output. Here we are sorting on this column in descending order and keeping the top results.  From the results we can see that 192.168.88.2 was communicating with 165.227.88.15 a large number of times. The dataset used is taken over a period of 24 hours so we can see that 108,856 connections divided by 86,400 seconds in a day means over 1 connection was sent every second on average. Furthermore, we know that these connections were made over UDP port 53, which is normally DNS. Typically, DNS results are cached in a number of places including:\n operating system\u0026rsquo;s local cache local or remote DNS recursive resolver  It is suspicious that DNS requests were being made so frequently. This could be a result of a misconfiguration or misbehaving software. Or it could indicate malicious software. Assuming we are familiar with the network configuration, we should be able to quickly tell that 165.227.88.15 is not an IP of a DNS server we recognize, which makes this traffic even more suspicious.\nTotal Data Transferred Large data transfers out of a network can indicate data exfiltration. Databases containing sensitive information, intellectual property in the form of images, videos, PDFs, or other binary formats are often targets for attackers. Each of these can consist of large amounts of information that can then be detected when transferred out of your network.\nThe following command will display the total number of bytes sent from the IP address in column 1 to the IP address in column 2. When the IP address in column 1 is an internal address and column 2 an external address, it means that this data was exfiltrated out of your network.\ncat conn.log | zeek-cut id.orig_h id.resp_h orig_bytes | awk 'BEGIN{ FS=\u0026quot;\\t\u0026quot; } { arr[$1 FS $2] += $3 } END{ for (key in arr) printf \u0026quot;%s%s%s\\n\u0026quot;, key, FS, arr[key] }' | sort -nrk 3 | head  192.168.88.2\t165.227.88.15\t6723739 10.55.100.111\t23.38.115.36\t981527 10.55.100.111\t34.233.92.30\t958540 10.55.100.111\t24.220.113.56\t778452 10.55.100.111\t24.220.113.58\t775648 10.55.100.111\t23.52.163.40\t734881 10.55.100.100\t23.38.115.36\t705408 10.55.100.103\t134.170.58.189\t637453 10.55.100.111\t23.63.220.157\t618329 10.55.100.105\t23.38.115.36\t615374   cat conn.log | zeek-cut id.orig_h id.resp_h orig_bytes - We\u0026rsquo;re taking Zeek\u0026rsquo;s conn.log and only keeping the source IP, destination IP, bytes sent by the source IP. awk - The following explains the pieces of the awk script.  BEGIN{ FS=\u0026quot;\\t\u0026quot; } - Set the FS (field separator) variable to a tab character. This is what is separating columns in our Zeek logs as well as what we want to use in our output. BEGIN means this instruction is only executed one time, before any data is processed. { arr[$1 FS $2] += $3 } - Creates an array (named arr). The important part here is that we are using the concatenation of the source and destination IPs as our array key. Which means that this command will add up the bytes for each pair of IPs. awk executes this instruction repeatedly for every line of data. END{ for (key in arr) printf \u0026quot;%s%s%s\\n\u0026quot;, key, FS, arr[key] } - Here we are looping through all the elements in the array and printing out the results. END signifies that awk only executes this instruction one time, after processing all the data.  sort -nrk 3 | head - The number of connections is the 3rd and final column printed in the output. Here we are sorting on this column in descending order and keeping the top results.  You can also modify the command to get the total amount of data sent in both directions.\ncat conn.log | zeek-cut id.orig_h id.resp_h orig_bytes resp_bytes | awk 'BEGIN{ FS=\u0026quot;\\t\u0026quot; } { arr[$1 FS $2] += $3+$4 } END{ for (key in arr) printf \u0026quot;%s%s%s\\n\u0026quot;, key, FS, arr[key] }' | sort -nrk 3 | head  10.55.100.111\t162.252.74.5\t2027554933 10.55.100.103\t13.107.4.50\t91904287 10.55.100.111\t24.220.113.59\t29008351 10.55.100.110\t72.21.81.240\t23388664 10.55.100.106\t40.77.228.30\t21117996 10.55.100.111\t23.38.115.36\t19471375 10.55.100.107\t13.107.4.50\t17989779 10.55.100.110\t23.38.115.36\t17960372 10.55.100.106\t13.107.4.50\t17653174 10.55.100.100\t13.107.4.50\t17528299  Uncommon User Agent Strings Attacker tools which send HTTP traffic will often include a User Agent header (UA) in the HTTP request. Many tools (e.g. nikto) will have a custom UA that identifies the tool. If an attacker forgets to set a custom value it should be a red flag to have such UA appear on your network. Even tools which have their default value set to a common web browser will sometimes make typos, such as including an extra space, that likewise will appear as anomalies. One final reason to look at UA\u0026rsquo;s is that if your network has consistent patching then you should have relatively few unique UA strings as each system will have identical browsers and versions. A unique value in this case could indicate: a system with missing patches, a user installing unauthorized software, or an attacker attempting to blend in by choosing a common UA but it doesn\u0026rsquo;t match what\u0026rsquo;s on your network.\nThis command uses Zeek\u0026rsquo;s http.log file to find all UA strings, and count how many of each appear in connections.\ncat http.log | zeek-cut user_agent | sort | uniq -c | sort -n | head   1 client connection 1 Windows-Update-Agent/7.9.9600.18756 Client-Protocol/1.21 2 Microsoft-CryptoAPI/6.3 9 Windows-Update-Agent/10.0.10011.16384 Client-Protocol/1.40 12 OfficeClickToRun 25 Microsoft BITS/7.8 40 MICROSOFT_DEVICE_METADATA_RETRIEVAL_CLIENT 44 Mozilla/5.0 (Windows NT 10.0; Win64; x64; Trident/7.0; rv:11.0) like Gecko 48 Mozilla/4.0 (compatible; FCT 5.6.0; Windows NT 5.1) 374 Microsoft-Delivery-Optimization/10.0  Next, we can investigate the unique UA strings to find out who was making the request and where the request was going. In both of the below cases, we are pulling out the source and destination IPs from the Zeek log, along with the HTTP Host header and the requested URI. Finally, we include the UA string so that we can filter out only the UAs we found above.\ncat http.log | zeek-cut id.orig_h id.resp_h host uri user_agent | grep 'client connection'  10.55.200.10\t191.239.52.100\ttele.trafficmanager.net\t/{AA35F099-DF2E-4104-8F15-FCB887FB32F3}\tclient connection  This appears to be a tracking server of some kind. Some quick reconnaissance on the domain shows that it belongs to Microsoft. We can tentatively assume that this is benign metrics tracking.\ncat http.log | zeek-cut id.orig_h id.resp_h host uri user_agent | grep 'Windows-Update-Agent/7.9.9600.18756 Client-Protocol/1.21'  10.55.200.10\t52.183.118.171\tstatsfe2.update.microsoft.com\t/ReportingWebService/ReportingWebService.asmx\tWindows-Update-Agent/7.9.9600.18756 Client-Protocol/1.21  In this case, the domain is also pretty clearly a Microsoft property, and we can verify that Microsoft also owns the destination IP. This appears to be part of the normal update process, but since it is a unique UA string the next step might be to investigate the system 10.55.200.10 to see why it is not running the same version of software as all the other systems.\nNote that the Host header can be spoofed in HTTP requests so you should not rely on this alone. You should verify in the DNS logs that the domain listed there actually resolved to the IP address in the HTTP log. Furthermore, the destination IP address may change and no longer be associated with that domain in this age of cloud computing. It is quite common for services to rotate or recycle IP addresses for a given domain name.\n"},{"uri":"/","title":"Introduction","tags":[],"description":"","content":" Threat Hunting Labs Introduction These are a series of labs that cover different types of analysis that can be done on network data when threat hunting. You can do these in any order and you can jump around individual labs to try out the tools or methods that interest you. That being said, here is our suggested order:\n Long Connections Beacons DNS Outliers  Each of these labs works off of a packet capture which is located in your home directory at ~/sample.pcap. You can copy and paste the commands easily if you work from your home directory as well.\nPlease note that this file is over 3GB and many of the tools that work off this pcap will take a couple of minutes to finish.\nYou can use the Basic Tool Usage guide as a reference for common tasks if a tool is unfamiliar to you.\nIf you enjoy these labs and are interested in learning more about network threat hunting, you can:\n Read articles on our Active Countermeasures Blog (or Subscribe for email updates) Sign up for our free Network Threat Hunter Training which consists of a series of recorded hour long videos  Thank you for participating!\n"},{"uri":"/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"/tags/","title":"Tags","tags":[],"description":"","content":""}]